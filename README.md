# 0. 准备工作
- 利用conda激活一个新的虚拟环境
  
```bash
conda create -n rl_lunar python=3.10 -y
conda activae rl_lunar
```

- 安装系统及依赖（SWIG）用于编译Box2D 物理引擎必须的工具
```bash
conda install -c conda-forge swig
```

- 现在 swig 准备好了，我们用 pip 安装剩下的强化学习库。虽然 Conda 也可以装 Python 包，但 gymnasium 和 stable-baselines3 更新很快，用 pip 能装到最新版且不容易出错。
```bash
pip install "gymnasium[box2d]" stable-baselines3
```

- 安装 TensorBoard 这个工具，用于记录日志&训练曲线
```bash
pip install tensorboard
```

# 1. 实现目标
如图所示，我们想让紫色的登月舱安全降落在两个黄色旗帜中间的平台（坐标 0,0）。
<img width="603" height="431" alt="截屏2025-12-03 14 18 23" src="https://github.com/user-attachments/assets/6328522e-c270-445a-82d8-e86754ffc8da" />
- 评分系统（Reward）：
  - 坠毁（Crash）： -100分（这是最大的惩罚，AI 最先学会的就是“别死”）。
  - 安全着陆： +100 ~ +140分（取决于着陆时的速度，速度越慢分越高）。
  - 腿部接触地面： +10分（每条腿10分，两条腿都踩实了就是20分，鼓励平稳）。
  - 喷射引擎： 扣分（每次喷火都会扣一点点分，大概 -0.03/帧）。这模拟了节省燃料。如果一直在空中悬停不下来，分会被扣光。
- 胜利标准： 官方定义是平均分超过 200分 就算“完全解决”。

# 2. 训练日志
- 调用日志指令，利用下面的指令会输出一个网址，里面存放着训练曲线
```bash
  tensorboard --logdir=logs
```
- 图解
<img width="953" height="486" alt="截屏2025-12-03 14 39 06" src="https://github.com/user-attachments/assets/c052e7f0-875f-4436-8051-34061e01d5a1" />

- 左图平均回合长度（效率指标）
  - Y轴 (Length): 游戏进行的帧数。
  - 曲线走势： 一路下降。
        - 刚开始 (450+): AI 还在犹豫。它可能在空中悬停很久不敢落，或者左右摇摆修正不好方向，导致时间很长。
        - 后来 (250左右): 随着训练进行，它变得越来越果断。
  - 解读： 当“分数上升”且“时长下降”时，意味着 AI 并没有通过“一直飞在空中不落地”来骗分，而是学会了快速、精准地直奔目标。这显示了极高的自信度。
- 右图平均奖励/分数（核心成绩单）
  - X轴 (Steps): 训练步数。你可以看到大概在 960k (接近100万) 步的时候。
  - Y轴 (Reward): 分数。
        - 曲线走势：
          - 极速爬升： 曲线从一开始就一路狂飙，没有任何卡顿。\n
          - 突破 200分 (关键点)： 在大概 40万-50万步的时候，曲线突破了 200分。在 LunarLander 中，200分被定义为“解决了这个问题”（Solved）。也就是说，从那一刻起，它已经是个合格的驾驶员了。
          - 最终得分 273.3： 注意看右下角的数值 Value: 273.3031。
          - 这分有多高？ 这个游戏的理论极限大约在 280-300 分左右（意味着极度省油 + 完美中心着陆）。
          - 你的成绩： 273 分属于大师级表现。它不仅每次都能落在旗帜中间，而且动作极其丝滑，几乎不浪费一滴燃料。
  
